{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据下载地址Display Advertising Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/criteo-display-ad-challenge/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "预处理Criteo数据集。该数据集用于显示广告挑战（https://www.kaggle.com/c/criteo-display-ad-challenge）。\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import click\n",
    "import random\n",
    "import collections\n",
    "\n",
    "# 有13个整数特征和26个分类特征\n",
    "continous_features = range(1, 14)\n",
    "categorial_features = range(14, 40)\n",
    "\n",
    "# 裁剪整数特征。每个整数特征的裁剪点来自于每个特征中总价值的95%分位数\n",
    "continous_clip = [20, 600, 100, 50, 64000, 500, 100, 50, 500, 10, 10, 10, 50]\n",
    "\n",
    "\n",
    "class CategoryDictGenerator:\n",
    "    \"\"\"\n",
    "    为每个分类特征生成字典\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_feature):\n",
    "        \"\"\"\n",
    "        初始化函数\n",
    "\n",
    "        参数：\n",
    "            num_feature (int): 特征数量\n",
    "\n",
    "        返回：\n",
    "            无\n",
    "\n",
    "        说明：\n",
    "            初始化时，创建指定数量的字典，并将每个字典初始化为一个空的 defaultdict(int)\n",
    "        \"\"\"\n",
    "        self.dicts = []\n",
    "        # 设置属性 num_feature\n",
    "        self.num_feature = num_feature\n",
    "        # 初始化 collections.defaultdict(int) 列表\n",
    "        for i in range(0, num_feature):\n",
    "            self.dicts.append(collections.defaultdict(int))\n",
    "\n",
    "    def build(self, datafile, categorial_features, cutoff=0):\n",
    "        \"\"\"\n",
    "        构建字典函数\n",
    "\n",
    "        参数：\n",
    "            datafile (str): 数据文件路径\n",
    "            categorial_features (list): 类别特征列表\n",
    "            cutoff (int): 字典中条目的最小出现次数\n",
    "\n",
    "        返回：\n",
    "            无\n",
    "\n",
    "        说明：\n",
    "            构建每个特征的字典，包含满足最小出现次数的条目\n",
    "        \"\"\"\n",
    "\n",
    "        # 打开数据文件，以只读模式\n",
    "        with open(datafile, 'r') as f:\n",
    "            # 遍历数据文件的每一行\n",
    "            for line in f:\n",
    "                # 移除行末的换行符并将行拆分为特征列表\n",
    "                features = line.rstrip('\\n').split('\\t')\n",
    "                # 遍历每个特征\n",
    "                for i in range(0, self.num_feature):\n",
    "                    # 如果特征值不为空\n",
    "                    if features[categorial_features[i]]!= '':\n",
    "                        # 增加相应特征字典中的计数\n",
    "                        self.dicts[i][features[categorial_features[i]]] += 1\n",
    "        # 遍历每个特征字典\n",
    "        for i in range(0, self.num_feature):\n",
    "            # 过滤掉字典中计数小于截止值的项\n",
    "            self.dicts[i] = filter(lambda x: x[1] >= cutoff,\n",
    "                                   self.dicts[i].items())\n",
    "            # 根据计数对字典中的项进行排序，先按计数降序，再按键升序\n",
    "            self.dicts[i] = sorted(self.dicts[i], key=lambda x: (-x[1], x[0]))\n",
    "            # 获取排序后的键值对并解压为vocabs和其他\n",
    "            vocabs, _ = list(zip(*self.dicts[i]))\n",
    "            # 将vocabs转换为字典，并为每个唯一的vocab分配一个从1开始的整数ID\n",
    "            self.dicts[i] = dict(zip(vocabs, range(1, len(vocabs) + 1)))\n",
    "            # 为每个字典添加一个特殊的'<unk>'键，并将其值设置为0\n",
    "            self.dicts[i]['<unk>'] = 0\n",
    "\n",
    "    def gen(self, idx, key):\n",
    "        \"\"\"\n",
    "        生成对应索引和键的值\n",
    "\n",
    "        参数：\n",
    "            idx (int): 字典索引\n",
    "            key (str): 要查找的值\n",
    "\n",
    "        返回：\n",
    "            int: 如果键存在于字典中，则返回其值；否则，返回代表未知的数字\n",
    "\n",
    "        说明：\n",
    "            如果键不存在于字典中，返回 0，代表未知的数字\n",
    "        \"\"\"\n",
    "        # 如果键不在字典中，则获取字典中键为<unk>的值\n",
    "        if key not in self.dicts[idx]:\n",
    "            res = self.dicts[idx]['<unk>']\n",
    "        # 否则获取字典中当前键对应的值\n",
    "        else:\n",
    "            res = self.dicts[idx][key]\n",
    "        # 返回结果\n",
    "        return res\n",
    "\n",
    "    def dicts_sizes(self):\n",
    "        \"\"\"\n",
    "        获取字典大小\n",
    "\n",
    "        参数：\n",
    "            无\n",
    "\n",
    "        返回：\n",
    "            list: 包含每个字典大小的列表\n",
    "\n",
    "        说明：\n",
    "            返回一个列表，其中每个元素是对应的字典的大小\n",
    "        \"\"\"\n",
    "        return [len(self.dicts[idx]) for idx in range(0, self.num_feature)]\n",
    "\n",
    "\n",
    "class ContinuousFeatureGenerator:\n",
    "    \"\"\"\n",
    "    裁剪连续特征。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_feature):\n",
    "        self.num_feature = num_feature\n",
    "\n",
    "    def build(self, datafile, continous_features):\n",
    "        # 打开数据文件，以只读模式\n",
    "        with open(datafile, 'r') as f:\n",
    "            # 读取文件中的每一行数据\n",
    "            for line in f:\n",
    "                # 去除每行数据结尾的换行符，并按制表符分割成特征列表\n",
    "                features = line.rstrip('\\n').split('\\t')\n",
    "                # 遍历所有连续型特征\n",
    "                for i in range(0, self.num_feature):\n",
    "                    # 获取当前连续型特征的值\n",
    "                    val = features[continous_features[i]]\n",
    "                    # 如果值存在且不为空\n",
    "                    if val!= '':\n",
    "                        # 将值转换为整数\n",
    "                        val = int(val)\n",
    "                        # 如果值大于当前连续型特征的最大值\n",
    "                        if val > continous_clip[i]:\n",
    "                            # 将值设置为当前连续型特征的最大值\n",
    "                            val = continous_clip[i]\n",
    "\n",
    "    def gen(self, idx, val):\n",
    "        # 如果值为空字符串，则返回 0.0\n",
    "        if val == '':\n",
    "            return 0.0\n",
    "        # 将值转换为浮点数\n",
    "        val = float(val)\n",
    "        # 返回转换后的值\n",
    "        return val\n",
    "\n",
    "\n",
    "# @click.command(\"preprocess\")\n",
    "# @click.option(\"--datadir\", type=str, help=\"Path to raw criteo dataset\")\n",
    "# @click.option(\"--outdir\", type=str, help=\"Path to save the processed data\")\n",
    "def preprocess(datadir, outdir, num_train_sample = 10000, num_test_sample = 10000):\n",
    "    \"\"\"\n",
    "    所有13个整数特征被归一化为连续值，这些连续特征被合并为一个维度为13的向量。\n",
    "    26个分类特征中的每一个都进行了one-hot编码，所有的one-hot向量被合并为一个稀疏的二进制向量。\n",
    "    \"\"\"\n",
    "\n",
    "    # 初始化连续特征生成器，参数为连续特征的数量\n",
    "    dists = ContinuousFeatureGenerator(len(continous_features))\n",
    "    # 构建特征字典，参数为数据文件路径和连续特征列表\n",
    "    dists.build(os.path.join(datadir, 'train.txt'), continous_features)\n",
    "\n",
    "    # 初始化类别字典生成器，参数为类别特征的数量\n",
    "    dicts = CategoryDictGenerator(len(categorial_features))\n",
    "    # 构建类别字典，参数为数据文件路径、类别特征列表和截断值\n",
    "    dicts.build(os.path.join(datadir, 'train.txt'), categorial_features, cutoff=200)\n",
    "\n",
    "    # 获取类别字典的大小\n",
    "    dict_sizes = dicts.dicts_sizes()\n",
    "    # 打开输出路径下的文件，若没有该文件则创建新文件，并将文件句柄保存到变量 feature_sizes 中\n",
    "    with open(os.path.join(outdir, 'feature_sizes.txt'), 'w') as feature_sizes:\n",
    "        # 初始化一个列表，其长度等于连续特征的数量加上类别字典的大小\n",
    "        sizes = [1] * len(continous_features) + dict_sizes\n",
    "        # 将列表中的每个元素转换为字符串\n",
    "        sizes = [str(i) for i in sizes]\n",
    "        # 将列表元素用逗号分隔，连接成一个字符串\n",
    "        feature_sizes.write(','.join(sizes))\n",
    "\n",
    "    # 设置随机数种子，确保每次运行的随机性是一致的，方便代码复现\n",
    "    random.seed(0)\n",
    "\n",
    "    # 保存用于训练的数据。\n",
    "\n",
    "    # 打开位于 outdir 目录下的 train.txt 文件，准备写入数据\n",
    "    with open(os.path.join(outdir, 'train.txt'), 'w') as out_train:\n",
    "        # 同时，打开位于 datadir 目录下的 train.txt 文件，准备读取数据\n",
    "        with open(os.path.join(datadir, 'train.txt'), 'r') as f:\n",
    "            # 初始化计数器 k，用于记录已处理的行数\n",
    "            k = 0\n",
    "            # 遍历文件 f 中的每一行\n",
    "            for line in f:\n",
    "                # 移除行尾的换行符，并拆分为特征列表\n",
    "                features = line.rstrip('\\n').split('\\t')\n",
    "\n",
    "                # 初始化连续特征值列表\n",
    "                continous_vals = []\n",
    "                # 遍历所有连续特征\n",
    "                for i in range(0, len(continous_features)):\n",
    "                    # 根据特征索引和当前行的数据，生成连续特征值\n",
    "                    val = dists.gen(i, features[continous_features[i]])\n",
    "                    # 将连续特征值格式化为小数点后 6 位，并进行处理\n",
    "                    continous_vals.append(\"{0:.6f}\".format(val).rstrip('0').rstrip('.'))\n",
    "\n",
    "                # 初始化类别特征值列表\n",
    "                categorial_vals = []\n",
    "                # 遍历所有类别特征\n",
    "                for i in range(0, len(categorial_features)):\n",
    "                    # 根据特征索引和当前行的数据，生成类别特征值\n",
    "                    val = dicts.gen(i, features[categorial_features[i]])\n",
    "                    # 将类别特征值转换为字符串\n",
    "                    categorial_vals.append(str(val))\n",
    "\n",
    "                # 将连续特征值列表转换为逗号分隔的字符串\n",
    "                continous_vals = ','.join(continous_vals)\n",
    "                # 将类别特征值列表转换为逗号分隔的字符串\n",
    "                categorial_vals = ','.join(categorial_vals)\n",
    "                # 获取当前行的标签值\n",
    "                label = features[0]\n",
    "                # 将连续特征值、类别特征值和标签值连接为一个字符串，并写入到 out_train 文件中\n",
    "                out_train.write(','.join([continous_vals, categorial_vals, label]) + '\\n')\n",
    "\n",
    "                # 更新计数器 k\n",
    "                k += 1\n",
    "                # 如果处理的行数达到或超过指定的 num_train_sample，则停止循环\n",
    "                if k >= num_train_sample:\n",
    "                    break\n",
    "\n",
    "    with open(os.path.join(outdir, 'test.txt'), 'w') as out:\n",
    "        with open(os.path.join(datadir, 'test.txt'), 'r') as f:\n",
    "            k = 0\n",
    "            for line in f:\n",
    "                features = line.rstrip('\\n').split('\\t')\n",
    "\n",
    "                continous_vals = []\n",
    "                for i in range(0, len(continous_features)):\n",
    "                    val = dists.gen(i, features[continous_features[i] - 1])\n",
    "                    continous_vals.append(\"{0:.6f}\".format(val).rstrip('0')\n",
    "                                         .rstrip('.'))\n",
    "                categorial_vals = []\n",
    "                for i in range(0, len(categorial_features)):\n",
    "                    val = dicts.gen(i, features[categorial_features[\n",
    "                        i] - 1])\n",
    "                    categorial_vals.append(str(val))\n",
    "\n",
    "                continous_vals = ','.join(continous_vals)\n",
    "                categorial_vals = ','.join(categorial_vals)\n",
    "                out.write(','.join([continous_vals, categorial_vals]) + '\\n')\n",
    "                k += 1\n",
    "                if k >= num_test_sample:\n",
    "                    break\n",
    "\n",
    "preprocess('./data/raw', './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/cloudide/workspace/All-in-One/DeepFM/init.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/cloudide/workspace/All-in-One/DeepFM/init.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m loader_val \u001b[39m=\u001b[39m DataLoader(val_data, batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/cloudide/workspace/All-in-One/DeepFM/init.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m             sampler\u001b[39m=\u001b[39msampler\u001b[39m.\u001b[39mSubsetRandomSampler(\u001b[39mrange\u001b[39m(Num_train, \u001b[39m10000\u001b[39m)))\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/cloudide/workspace/All-in-One/DeepFM/init.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# 加载特征大小数据\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/cloudide/workspace/All-in-One/DeepFM/init.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# 从文本文件加载特征大小数据，每行数据用逗号分隔，得到一个字符串列表\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://icube%2Bicube/cloudide/workspace/All-in-One/DeepFM/init.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m feature_sizes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mloadtxt(\u001b[39m'\u001b[39m\u001b[39m./data/feature_sizes.txt\u001b[39m\u001b[39m'\u001b[39m, delimiter\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/cloudide/workspace/All-in-One/DeepFM/init.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# 将字符串列表转换为整数列表\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/cloudide/workspace/All-in-One/DeepFM/init.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m feature_sizes \u001b[39m=\u001b[39m [\u001b[39mint\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m feature_sizes]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:988\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[39m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[39mif\u001b[39;00m info\u001b[39m.\u001b[39mpydev_state \u001b[39m==\u001b[39m STATE_SUSPEND:\n\u001b[0;32m--> 988\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_wait_suspend(thread, frame, event, arg)\n\u001b[1;32m    989\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrace_dispatch\n\u001b[1;32m    990\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_wait_suspend\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_args[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdo_wait_suspend(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "from model.DeepFM import DeepFM\n",
    "from data.dataset import CriteoDataset\n",
    "\n",
    "# 设置训练集的大小，900000 条数据用于训练，10000 条数据用于验证，总共 1000000 条数据\n",
    "\n",
    "Num_train = 9000\n",
    "\n",
    "# 加载数据\n",
    "# 从指定目录加载训练数据\n",
    "train_data = CriteoDataset('./data', train=True)\n",
    "# DataLoader 用于将数据加载为可迭代的批次，批量大小设置为 16\n",
    "# 从训练数据中随机选择子集范围进行采样\n",
    "loader_train = DataLoader(train_data, batch_size=16,\n",
    "          sampler=sampler.SubsetRandomSampler(range(Num_train)))\n",
    "\n",
    "# 加载验证数据\n",
    "val_data = CriteoDataset('./data', train=False)\n",
    "# DataLoader 用于将数据加载为可迭代的批次，批量大小设置为 16\n",
    "# 从训练数据中随机选择子集范围进行采样\n",
    "loader_val = DataLoader(val_data, batch_size=16,\n",
    "        sampler=sampler.SubsetRandomSampler(range(Num_train, 10000)))\n",
    "\n",
    "# 加载特征大小数据\n",
    "# 从文本文件加载特征大小数据，每行数据用逗号分隔，得到一个字符串列表\n",
    "feature_sizes = np.loadtxt('./data/feature_sizes.txt', delimiter=',')\n",
    "# 将字符串列表转换为整数列表\n",
    "feature_sizes = [int(x) for x in feature_sizes]\n",
    "# 打印特征大小\n",
    "print(feature_sizes)\n",
    "\n",
    "# 创建 DeepFM 模型\n",
    "# 根据特征大小列表创建 DeepFM 模型\n",
    "model = DeepFM(feature_sizes, use_cuda=False)\n",
    "\n",
    "# 创建优化器\n",
    "# 使用 Adam 优化器，学习率设置为 1e-4，权重衰减设置为 0.0\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.0)\n",
    "model.fit(loader_train, loader_val, optimizer, epochs=5, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
